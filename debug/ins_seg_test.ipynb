{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from network.gap_layers import *\n",
    "from datasets.datasets_pair import *\n",
    "import functools\n",
    "from network.sym_v1 import *\n",
    "from network.flownet3d import *\n",
    "from network.gpv_layers import *\n",
    "from loss.utils import *\n",
    "from visu.utils import *\n",
    "from network.utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets.GAPartNet.misc.info import OBJECT_NAME2ID, PART_ID2NAME, PART_NAME2ID, get_symmetry_matrix\n",
    "from datasets.GAPartNet.dataset.instances import Instances\n",
    "from epic_ops.reduce import segmented_maxpool\n",
    "from network.gap_grouping_utils import (apply_nms, cluster_proposals, compute_ap,\n",
    "                               compute_npcs_loss, filter_invalid_proposals,\n",
    "                               get_gt_scores, segmented_voxelize)\n",
    "from einops import rearrange, repeat\n",
    "from epic_ops.iou import batch_instance_seg_iou\n",
    "from loss.utils import focal_loss, dice_loss, pixel_accuracy, mean_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/16T/zhangran/GAPartNet_re_rendered/train\"\n",
    "test_intra_dir = \"/16T/zhangran/GAPartNet_re_rendered/test_intra\"\n",
    "test_inter_dir = \"/16T/zhangran/GAPartNet_re_rendered/test_inter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_test_intra, dataset_test_inter = get_datasets(root_dir, test_intra_dir, test_inter_dir, voxelization=False, shot=True, choose_category=None, max_points=20000, augmentation=False)\n",
    "dataloader_train, dataloader_test_intra, dataloader_test_inter = get_dataloaders(dataset_train, dataset_test_intra, dataset_test_inter, num_workers=0, batch_size=8)\n",
    "print(len(dataset_train), len(dataset_test_intra), len(dataset_test_inter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecon_feat(nn.Module):\n",
    "    def __init__(self, gcn_n_num, gcn_sup_num):\n",
    "        super(FaceRecon_feat, self).__init__()\n",
    "        self.neighbor_num = gcn_n_num\n",
    "        self.support_num = gcn_sup_num\n",
    "\n",
    "        # 3D convolution for point cloud\n",
    "        self.conv_0 = Conv_surface(kernel_num=128, support_num=self.support_num)\n",
    "        self.conv_1 = Conv_layer(128, 128, support_num=self.support_num)\n",
    "        self.pool_1 = Pool_layer(pooling_rate=4, neighbor_num=4)\n",
    "        self.conv_2 = Conv_layer(128, 256, support_num=self.support_num)\n",
    "        self.conv_3 = Conv_layer(256, 256, support_num=self.support_num)\n",
    "        self.pool_2 = Pool_layer(pooling_rate=4, neighbor_num=4)\n",
    "        self.conv_4 = Conv_layer(256, 512, support_num=self.support_num)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self,\n",
    "                vertices: \"tensor (bs, vetice_num, 3)\", # type: ignore\n",
    "                # cat_id: \"tensor (bs, 1)\",\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Return: (bs, vertice_num, class_num)\n",
    "        \"\"\"\n",
    "\n",
    "        neighbor_index = get_neighbor_index(vertices, self.neighbor_num)\n",
    "        fm_0 = F.relu(self.conv_0(neighbor_index, vertices), inplace=True)\n",
    "\n",
    "        fm_1 = F.relu(self.bn1(self.conv_1(neighbor_index, vertices, fm_0).transpose(1, 2)).transpose(1, 2),\n",
    "                      inplace=True)\n",
    "        v_pool_1, fm_pool_1 = self.pool_1(vertices, fm_1)\n",
    "        neighbor_index = get_neighbor_index(v_pool_1,\n",
    "                                                  min(self.neighbor_num, v_pool_1.shape[1] // 8))\n",
    "        fm_2 = F.relu(self.bn2(self.conv_2(neighbor_index, v_pool_1, fm_pool_1).transpose(1, 2)).transpose(1, 2),\n",
    "                      inplace=True)\n",
    "        fm_3 = F.relu(self.bn3(self.conv_3(neighbor_index, v_pool_1, fm_2).transpose(1, 2)).transpose(1, 2),\n",
    "                      inplace=True)\n",
    "        v_pool_2, fm_pool_2 = self.pool_2(v_pool_1, fm_3)\n",
    "        neighbor_index = get_neighbor_index(v_pool_2, min(self.neighbor_num,\n",
    "                                                                v_pool_2.shape[1] // 8))\n",
    "        fm_4 = self.conv_4(neighbor_index, v_pool_2, fm_pool_2)\n",
    "        nearest_pool_1 = get_nearest_index(vertices, v_pool_1)\n",
    "        nearest_pool_2 = get_nearest_index(vertices, v_pool_2)\n",
    "        fm_2 = indexing_neighbor(fm_2, nearest_pool_1).squeeze(2)\n",
    "        fm_3 = indexing_neighbor(fm_3, nearest_pool_1).squeeze(2)\n",
    "        fm_4 = indexing_neighbor(fm_4, nearest_pool_2).squeeze(2)\n",
    "\n",
    "        feat = torch.cat([fm_0, fm_1, fm_2, fm_3, fm_4], dim=2)\n",
    "        '''\n",
    "        feat_face = torch.cat([fm_0, fm_1, fm_2, fm_3, fm_4], dim=2)\n",
    "        feat_face = torch.mean(feat_face, dim=1, keepdim=True)  # bs x 1 x channel\n",
    "        feat_face_re = feat_face.repeat(1, feat.shape[1], 1)\n",
    "        '''\n",
    "        return feat\n",
    "    \n",
    "class PoseNet9D_Only_R(nn.Module):\n",
    "    def __init__(self, feat_c_R=1280, R_c=4, gcn_n_num=10, gcn_sup_num=7, face_recon_c=6 * 5, obj_c=6, feat_face=768, feat_c_ts=1289, Ts_c=6):\n",
    "        super(PoseNet9D_Only_R, self).__init__()\n",
    "        self.rot_green = Rot_green(feat_c_R, R_c)\n",
    "        self.rot_red = Rot_red(feat_c_R, R_c)\n",
    "        self.face_recon = FaceRecon_feat(gcn_n_num, gcn_sup_num)\n",
    "        # self.ts = Pose_Ts(feat_c_ts, Ts_c)\n",
    "\n",
    "    def forward(self, points):\n",
    "        bs, p_num = points.shape[0], points.shape[1]\n",
    "        feat = self.face_recon(points - points.mean(dim=1, keepdim=True))\n",
    "        # rotation\n",
    "        green_R_vec = self.rot_green(feat.permute(0, 2, 1))  # b x 4\n",
    "        red_R_vec = self.rot_red(feat.permute(0, 2, 1))   # b x 4\n",
    "        # normalization\n",
    "        p_green_R = green_R_vec[:, 1:] / (torch.norm(green_R_vec[:, 1:], dim=1, keepdim=True) + 1e-6)\n",
    "        p_red_R = red_R_vec[:, 1:] / (torch.norm(red_R_vec[:, 1:], dim=1, keepdim=True) + 1e-6)\n",
    "        # sigmoid for confidence\n",
    "        f_green_R = F.sigmoid(green_R_vec[:, 0])\n",
    "        f_red_R = F.sigmoid(red_R_vec[:, 0])\n",
    "        # translation and size no need\n",
    "        return p_green_R, p_red_R, f_green_R, f_red_R\n",
    "\n",
    "class test_GPV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = PoseNet9D_Only_R(feat_c_R=1280)\n",
    "    \n",
    "    def forward(self, pc_list: List[PointCloudPair]):\n",
    "        points1 = torch.cat([pc.pc1.points.unsqueeze(0) for pc in pc_list], dim=0)  # pc_list is batch size\n",
    "        points2 = torch.cat([pc.pc2.points.unsqueeze(0) for pc in pc_list], dim=0)\n",
    "        p_green_R1, p_red_R1, f_green_R1, f_red_R1 = self.backbone(points1[:,0:2000, 0:3])\n",
    "        p_green_R2, p_red_R2, f_green_R2, f_red_R2 = self.backbone(points2[:,0:2000, 0:3])\n",
    "        return (p_green_R1, p_red_R1, f_green_R1, f_red_R1), (p_green_R2, p_red_R2, f_green_R2, f_red_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = {\n",
    "    dataset_train: root_dir,\n",
    "    dataset_test_inter: test_inter_dir,\n",
    "    dataset_test_intra: test_intra_dir\n",
    "}\n",
    "log_name = {\n",
    "    dataset_train: \"train\",\n",
    "    dataset_test_inter: \"inter\",\n",
    "    dataset_test_intra: \"intra\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelization_points(\n",
    "    points: torch.Tensor, voxel_size: Tuple[float, float, float]\n",
    "):\n",
    "    bs = points.shape[0]\n",
    "    voxel_features = []\n",
    "    voxel_coords = []\n",
    "    pc_voxel_ids = []\n",
    "    voxel_coords_ranges = []\n",
    "    for i in range(bs):\n",
    "        num_points = points.shape[1]\n",
    "        pt_xyz = points[i, :, :3]\n",
    "        points_range_min = pt_xyz.min(0)[0] - 1e-4\n",
    "        points_range_max = pt_xyz.max(0)[0] + 1e-4\n",
    "        voxel_feature, voxel_coord, _, pc_voxel_id = voxelize(\n",
    "            pt_xyz, points[i],\n",
    "            batch_offsets=torch.as_tensor([0, num_points], dtype=torch.int64, device = pt_xyz.device),\n",
    "            voxel_size=torch.as_tensor(voxel_size, device = pt_xyz.device),\n",
    "            points_range_min=torch.as_tensor(points_range_min, device = pt_xyz.device),\n",
    "            points_range_max=torch.as_tensor(points_range_max, device = pt_xyz.device),\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        voxel_features.append(voxel_feature)\n",
    "        voxel_coords.append(voxel_coord)\n",
    "        pc_voxel_ids.append(pc_voxel_id)\n",
    "        voxel_coords_range = (voxel_coord.max(0)[0] + 1).clamp(min=128, max=None)\n",
    "        voxel_coords_ranges.append(voxel_coords_range)\n",
    "        assert (pc_voxel_id >= 0).all()\n",
    "\n",
    "    # voxel_coords_range = (voxel_coords.max(0)[0] + 1).clamp(min=128, max=None)\n",
    "    # voxel_coords_range = voxel_coords_range.tolist()\n",
    "    batch_indices = torch.cat([\n",
    "        torch.full((point.shape[0],), i, dtype=torch.int32, device=\"cuda:0\")\n",
    "        for i, point in enumerate(points)\n",
    "    ], dim=0)\n",
    "    voxel_batch_indices = torch.cat([\n",
    "        torch.full((\n",
    "            voxel_coord.shape[0],), i, dtype=torch.int32, device=\"cuda:0\"\n",
    "        )\n",
    "        for i, voxel_coord in enumerate(voxel_coords)\n",
    "    ], dim=0)\n",
    "    voxel_features = torch.cat(voxel_features, dim=0)\n",
    "    voxel_coords = torch.cat(voxel_coords, dim=0)\n",
    "    voxel_coords = torch.cat([\n",
    "        voxel_batch_indices[:, None], voxel_coords\n",
    "    ], dim=-1)\n",
    "    pc_voxel_id = torch.cat(pc_voxel_ids, dim=0)\n",
    "    voxel_coords_range = torch.max(torch.stack(voxel_coords_ranges), dim=0)[0]\n",
    "    voxel_tensor = spconv.SparseConvTensor(\n",
    "        voxel_features, voxel_coords,\n",
    "        spatial_shape=voxel_coords_range.tolist(),\n",
    "        batch_size=bs\n",
    "    )\n",
    "    return voxel_tensor, pc_voxel_id, batch_indices\n",
    "\n",
    "class InsSegTest(nn.Module):\n",
    "    def __init__(self, backbone_type: str = \"SparseUNet\", num_part_classes = 10, channels = [16, 32, 48, 64, 80, 96, 112], block_repeat = 2):\n",
    "        super(InsSegTest, self).__init__()\n",
    "        self.num_part_classes = num_part_classes\n",
    "        self.backbone_type = backbone_type\n",
    "        self.ball_query_radius = 0.04\n",
    "        self.max_num_points_per_query = 50\n",
    "        self.min_num_points_per_proposal = 5 # 50 for scannet?\n",
    "        self.max_num_points_per_query_shift = 300\n",
    "        self.score_fullscale = 28\n",
    "        self.score_scale = 50\n",
    "        if backbone_type == \"SparseUNet\":\n",
    "            self.backbone = SparseUNet.build(6, channels, block_repeat, functools.partial(nn.BatchNorm1d, eps=1e-4, momentum=0.1))\n",
    "            fea_dim = 16\n",
    "        else:\n",
    "            raise NotImplementedError(\"backbone not implemented\")\n",
    "        norm_fn = functools.partial(nn.BatchNorm1d, eps=1e-4, momentum=0.1)\n",
    "        self.sem_seg_head = nn.Linear(16, self.num_part_classes)\n",
    "        self.offset_head = nn.Sequential(\n",
    "        nn.Linear(fea_dim, fea_dim),\n",
    "            norm_fn(fea_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(fea_dim, 3),\n",
    "        )\n",
    "        self.score_unet = SparseUNet.build(\n",
    "            fea_dim, channels[:2], block_repeat, norm_fn, without_stem=True\n",
    "        )\n",
    "        self.score_head = nn.Linear(fea_dim, self.num_part_classes - 1)\n",
    "        self.npcs_unet = SparseUNet.build(\n",
    "            fea_dim, channels[:2], block_repeat, norm_fn, without_stem=True\n",
    "        )\n",
    "        self.npcs_head = nn.Linear(fea_dim, 3 * (self.num_part_classes - 1))\n",
    "        \n",
    "        (\n",
    "            symmetry_matrix_1, symmetry_matrix_2, symmetry_matrix_3\n",
    "        ) = get_symmetry_matrix()\n",
    "        self.symmetry_matrix_1 = symmetry_matrix_1\n",
    "        self.symmetry_matrix_2 = symmetry_matrix_2\n",
    "        self.symmetry_matrix_3 = symmetry_matrix_3\n",
    "    \n",
    "    def forward(self, points, flow_data, instance_labels = None): # (bs,2048,3) + (bs,3,2048)\n",
    "        # 假设flow_data是backbone处理后的光流数据\n",
    "        flow_data = flow_data.permute(0, 2, 1)\n",
    "        inputs = torch.cat([points, flow_data], dim=2)\n",
    "        pc_features, batch_indices = self.forward_backbone(inputs)\n",
    "        cat_inputs = torch.cat([points[i] for i in range(points.shape[0])], dim=0)\n",
    "        pt_xyz = cat_inputs[:, :3]\n",
    "        sem_logits = self.sem_seg_head(pc_features)\n",
    "        sem_preds = torch.argmax(sem_logits.detach(), dim=-1)\n",
    "        offsets_preds = self.forward_offset(pc_features)\n",
    "        voxel_tensor, pc_voxel_id, proposals = self.proposal_clustering_and_revoxelize(\n",
    "            pt_xyz = pt_xyz,\n",
    "            batch_indices=batch_indices,\n",
    "            pt_features=pc_features,\n",
    "            sem_preds=sem_preds,\n",
    "            offset_preds=offsets_preds,\n",
    "            instance_labels=instance_labels,\n",
    "        )\n",
    "        if proposals is None:\n",
    "            return sem_preds, sem_logits, offsets_preds, None, None, None\n",
    "        \n",
    "        score_logits = self.forward_proposal_score(\n",
    "            voxel_tensor, pc_voxel_id, proposals\n",
    "        )\n",
    "        proposal_offsets_begin = proposals.proposal_offsets[:-1].long()\n",
    "        score_logits = score_logits.gather(\n",
    "            1, proposals.sem_preds[proposal_offsets_begin].long()[:, None] - 1\n",
    "        ).squeeze(1)\n",
    "        proposals.score_preds = score_logits.detach().sigmoid()\n",
    "        npcs_logits = self.forward_proposal_npcs(\n",
    "            voxel_tensor, pc_voxel_id\n",
    "        )\n",
    "        return sem_preds, sem_logits, offsets_preds, proposals, score_logits, npcs_logits\n",
    "    \n",
    "    def forward_backbone(self, inputs):\n",
    "        if self.backbone_type == \"SparseUNet\":\n",
    "            voxel_tensor, pc_voxel_id, batch_indices = voxelization_points(inputs, [0.01,0.01,0.01])\n",
    "            voxel_features = self.backbone(voxel_tensor)\n",
    "            pc_feature = voxel_features.features[pc_voxel_id]\n",
    "            return pc_feature, batch_indices\n",
    "        else:\n",
    "            raise NotImplementedError(\"backbone not implemented\")\n",
    "    \n",
    "    def forward_sem_seg(\n",
    "        self,\n",
    "        pc_feature: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        sem_logits = self.sem_seg_head(pc_feature)\n",
    "\n",
    "        return sem_logits\n",
    "    \n",
    "    def forward_offset(\n",
    "        self,\n",
    "        pc_feature: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        offset = self.offset_head(pc_feature)\n",
    "\n",
    "        return offset\n",
    "    \n",
    "    def forward_proposal_score(\n",
    "        self,\n",
    "        voxel_tensor: spconv.SparseConvTensor,\n",
    "        pc_voxel_id: torch.Tensor,\n",
    "        proposals: Instances,\n",
    "    ):\n",
    "        proposal_offsets = proposals.proposal_offsets\n",
    "        proposal_offsets_begin = proposal_offsets[:-1] # type: ignore\n",
    "        proposal_offsets_end = proposal_offsets[1:] # type: ignore\n",
    "\n",
    "        score_features = self.score_unet(voxel_tensor)\n",
    "        score_features = score_features.features[pc_voxel_id]\n",
    "        pooled_score_features, _ = segmented_maxpool(\n",
    "            score_features, proposal_offsets_begin, proposal_offsets_end\n",
    "        )\n",
    "        score_logits = self.score_head(pooled_score_features)\n",
    "\n",
    "        return score_logits\n",
    "    \n",
    "    def forward_proposal_npcs(\n",
    "        self,\n",
    "        voxel_tensor: spconv.SparseConvTensor,\n",
    "        pc_voxel_id: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        npcs_features = self.npcs_unet(voxel_tensor)\n",
    "        npcs_logits = self.npcs_head(npcs_features.features)\n",
    "        npcs_logits = npcs_logits[pc_voxel_id]\n",
    "\n",
    "        return npcs_logits\n",
    "    \n",
    "\n",
    "    def proposal_clustering_and_revoxelize(\n",
    "        self,\n",
    "        pt_xyz: torch.Tensor,\n",
    "        batch_indices: torch.Tensor,\n",
    "        pt_features: torch.Tensor,\n",
    "        sem_preds: torch.Tensor,\n",
    "        offset_preds: torch.Tensor,\n",
    "        instance_labels: Optional[torch.Tensor],\n",
    "    ):\n",
    "        device = pt_xyz.device\n",
    "        \n",
    "        if instance_labels is not None:\n",
    "            valid_mask = (sem_preds > 0) & (instance_labels >= 0)\n",
    "        else:\n",
    "            valid_mask = sem_preds > 0\n",
    "        \n",
    "        pt_xyz = pt_xyz[valid_mask]\n",
    "        batch_indices = batch_indices[valid_mask]\n",
    "        pt_features = pt_features[valid_mask]\n",
    "        sem_preds = sem_preds[valid_mask].int()\n",
    "        offset_preds = offset_preds[valid_mask]\n",
    "        if instance_labels is not None:\n",
    "            instance_labels = instance_labels[valid_mask]\n",
    "            \n",
    "        # get batch offsets (csr) from batch indices\n",
    "        _, batch_indices_compact, num_points_per_batch = torch.unique_consecutive(\n",
    "            batch_indices, return_inverse=True, return_counts=True\n",
    "        )\n",
    "        batch_indices_compact = batch_indices_compact.int()\n",
    "        batch_offsets = torch.zeros(\n",
    "            (num_points_per_batch.shape[0] + 1,), dtype=torch.int32, device=device\n",
    "        )\n",
    "        batch_offsets[1:] = num_points_per_batch.cumsum(0)\n",
    "        \n",
    "        # cluster proposals: dual set\n",
    "        sorted_cc_labels, sorted_indices = cluster_proposals(\n",
    "            pt_xyz, batch_indices_compact, batch_offsets, sem_preds,\n",
    "            self.ball_query_radius, self.max_num_points_per_query,\n",
    "        )\n",
    "\n",
    "        sorted_cc_labels_shift, sorted_indices_shift = cluster_proposals(\n",
    "            pt_xyz + offset_preds, batch_indices_compact, batch_offsets, sem_preds,\n",
    "            self.ball_query_radius, self.max_num_points_per_query_shift,\n",
    "        )\n",
    "        \n",
    "        # combine clusters\n",
    "        sorted_cc_labels = torch.cat([\n",
    "            sorted_cc_labels,\n",
    "            sorted_cc_labels_shift + sorted_cc_labels.shape[0],\n",
    "        ], dim=0)\n",
    "        sorted_indices = torch.cat([sorted_indices, sorted_indices_shift], dim=0)\n",
    "\n",
    "        # compact the proposal ids\n",
    "        _, proposal_indices, num_points_per_proposal = torch.unique_consecutive(\n",
    "            sorted_cc_labels, return_inverse=True, return_counts=True\n",
    "        )\n",
    "\n",
    "        # remove small proposals\n",
    "        valid_proposal_mask = (\n",
    "            num_points_per_proposal >= self.min_num_points_per_proposal\n",
    "        )\n",
    "        # proposal to point\n",
    "        valid_point_mask = valid_proposal_mask[proposal_indices]\n",
    "\n",
    "        sorted_indices = sorted_indices[valid_point_mask]\n",
    "        if sorted_indices.shape[0] == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        batch_indices = batch_indices[sorted_indices]\n",
    "        pt_xyz = pt_xyz[sorted_indices]\n",
    "        pt_features = pt_features[sorted_indices]\n",
    "        sem_preds = sem_preds[sorted_indices]\n",
    "        if instance_labels is not None:\n",
    "            instance_labels = instance_labels[sorted_indices]\n",
    "\n",
    "        # re-compact the proposal ids\n",
    "        proposal_indices = proposal_indices[valid_point_mask]\n",
    "        _, proposal_indices, num_points_per_proposal = torch.unique_consecutive(\n",
    "            proposal_indices, return_inverse=True, return_counts=True\n",
    "        )\n",
    "        num_proposals = num_points_per_proposal.shape[0]\n",
    "\n",
    "        # get proposal batch offsets\n",
    "        proposal_offsets = torch.zeros(\n",
    "            num_proposals + 1, dtype=torch.int32, device=device\n",
    "        )\n",
    "        proposal_offsets[1:] = num_points_per_proposal.cumsum(0)\n",
    "\n",
    "        # voxelization\n",
    "        voxel_features, voxel_coords, pc_voxel_id = segmented_voxelize(\n",
    "            pt_xyz, pt_features,\n",
    "            proposal_offsets, proposal_indices,\n",
    "            num_points_per_proposal,\n",
    "            self.score_fullscale, self.score_scale,\n",
    "        )\n",
    "        voxel_tensor = spconv.SparseConvTensor(\n",
    "            voxel_features, voxel_coords.int(),\n",
    "            spatial_shape=[self.score_fullscale] * 3,\n",
    "            batch_size=num_proposals,\n",
    "        )\n",
    "        if not (pc_voxel_id >= 0).all():\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            \n",
    "        proposals = Instances(\n",
    "            valid_mask=valid_mask,\n",
    "            sorted_indices=sorted_indices,\n",
    "            pt_xyz=pt_xyz,\n",
    "            batch_indices=batch_indices,\n",
    "            proposal_offsets=proposal_offsets,\n",
    "            proposal_indices=proposal_indices,\n",
    "            num_points_per_proposal=num_points_per_proposal,\n",
    "            sem_preds=sem_preds,\n",
    "            instance_labels=instance_labels,\n",
    "        )\n",
    "\n",
    "        return voxel_tensor, pc_voxel_id, proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsSegTest(\n",
       "  (backbone): SparseUNet(\n",
       "    (stem): SparseSequential(\n",
       "      (0): SubMConv3d(6, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "      (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (ublock): UBlock(\n",
       "      (encoder_blocks): SparseSequential(\n",
       "        (0): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): SparseSequential(\n",
       "        (0): SparseConv3d(16, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "        (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (ublock): UBlock(\n",
       "        (encoder_blocks): SparseSequential(\n",
       "          (0): ResBlock(\n",
       "            (shortcut): Identity()\n",
       "            (conv1): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv2): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResBlock(\n",
       "            (shortcut): Identity()\n",
       "            (conv1): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv2): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): SparseSequential(\n",
       "          (0): SparseConv3d(32, 48, kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "          (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (ublock): UBlock(\n",
       "          (encoder_blocks): SparseSequential(\n",
       "            (0): ResBlock(\n",
       "              (shortcut): Identity()\n",
       "              (conv1): SparseSequential(\n",
       "                (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (conv2): SparseSequential(\n",
       "                (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (shortcut): Identity()\n",
       "              (conv1): SparseSequential(\n",
       "                (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (conv2): SparseSequential(\n",
       "                (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SparseSequential(\n",
       "            (0): SparseConv3d(48, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (ublock): UBlock(\n",
       "            (encoder_blocks): SparseSequential(\n",
       "              (0): ResBlock(\n",
       "                (shortcut): Identity()\n",
       "                (conv1): SparseSequential(\n",
       "                  (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (conv2): SparseSequential(\n",
       "                  (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): ResBlock(\n",
       "                (shortcut): Identity()\n",
       "                (conv1): SparseSequential(\n",
       "                  (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (conv2): SparseSequential(\n",
       "                  (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): SparseSequential(\n",
       "              (0): SparseConv3d(64, 80, kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "            )\n",
       "            (ublock): UBlock(\n",
       "              (encoder_blocks): SparseSequential(\n",
       "                (0): ResBlock(\n",
       "                  (shortcut): Identity()\n",
       "                  (conv1): SparseSequential(\n",
       "                    (0): SubMConv3d(80, 80, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (conv2): SparseSequential(\n",
       "                    (0): SubMConv3d(80, 80, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (1): ResBlock(\n",
       "                  (shortcut): Identity()\n",
       "                  (conv1): SparseSequential(\n",
       "                    (0): SubMConv3d(80, 80, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (conv2): SparseSequential(\n",
       "                    (0): SubMConv3d(80, 80, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SparseSequential(\n",
       "                (0): SparseConv3d(80, 96, kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU()\n",
       "              )\n",
       "              (ublock): UBlock(\n",
       "                (encoder_blocks): SparseSequential(\n",
       "                  (0): ResBlock(\n",
       "                    (shortcut): Identity()\n",
       "                    (conv1): SparseSequential(\n",
       "                      (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (conv2): SparseSequential(\n",
       "                      (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): ResBlock(\n",
       "                    (shortcut): Identity()\n",
       "                    (conv1): SparseSequential(\n",
       "                      (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (conv2): SparseSequential(\n",
       "                      (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (downsample): SparseSequential(\n",
       "                  (0): SparseConv3d(96, 112, kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(112, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): ReLU()\n",
       "                )\n",
       "                (ublock): UBlock(\n",
       "                  (encoder_blocks): SparseSequential(\n",
       "                    (0): ResBlock(\n",
       "                      (shortcut): Identity()\n",
       "                      (conv1): SparseSequential(\n",
       "                        (0): SubMConv3d(112, 112, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                        (1): BatchNorm1d(112, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      )\n",
       "                      (conv2): SparseSequential(\n",
       "                        (0): SubMConv3d(112, 112, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                        (1): BatchNorm1d(112, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      )\n",
       "                    )\n",
       "                    (1): ResBlock(\n",
       "                      (shortcut): Identity()\n",
       "                      (conv1): SparseSequential(\n",
       "                        (0): SubMConv3d(112, 112, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                        (1): BatchNorm1d(112, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      )\n",
       "                      (conv2): SparseSequential(\n",
       "                        (0): SubMConv3d(112, 112, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                        (1): BatchNorm1d(112, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (upsample): SparseSequential(\n",
       "                  (0): SparseInverseConv3d(112, 96, kernel_size=[2, 2, 2], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): ReLU()\n",
       "                )\n",
       "                (decoder_blocks): SparseSequential(\n",
       "                  (0): ResBlock(\n",
       "                    (shortcut): SparseSequential(\n",
       "                      (0): SubMConv3d(192, 96, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (conv1): SparseSequential(\n",
       "                      (0): SubMConv3d(192, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (conv2): SparseSequential(\n",
       "                      (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): ResBlock(\n",
       "                    (shortcut): Identity()\n",
       "                    (conv1): SparseSequential(\n",
       "                      (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (conv2): SparseSequential(\n",
       "                      (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                      (1): BatchNorm1d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (upsample): SparseSequential(\n",
       "                (0): SparseInverseConv3d(96, 80, kernel_size=[2, 2, 2], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU()\n",
       "              )\n",
       "              (decoder_blocks): SparseSequential(\n",
       "                (0): ResBlock(\n",
       "                  (shortcut): SparseSequential(\n",
       "                    (0): SubMConv3d(160, 80, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (conv1): SparseSequential(\n",
       "                    (0): SubMConv3d(160, 80, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (conv2): SparseSequential(\n",
       "                    (0): SubMConv3d(80, 80, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (1): ResBlock(\n",
       "                  (shortcut): Identity()\n",
       "                  (conv1): SparseSequential(\n",
       "                    (0): SubMConv3d(80, 80, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (conv2): SparseSequential(\n",
       "                    (0): SubMConv3d(80, 80, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                    (1): BatchNorm1d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (upsample): SparseSequential(\n",
       "              (0): SparseInverseConv3d(80, 64, kernel_size=[2, 2, 2], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "            )\n",
       "            (decoder_blocks): SparseSequential(\n",
       "              (0): ResBlock(\n",
       "                (shortcut): SparseSequential(\n",
       "                  (0): SubMConv3d(128, 64, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (conv1): SparseSequential(\n",
       "                  (0): SubMConv3d(128, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (conv2): SparseSequential(\n",
       "                  (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): ResBlock(\n",
       "                (shortcut): Identity()\n",
       "                (conv1): SparseSequential(\n",
       "                  (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (conv2): SparseSequential(\n",
       "                  (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                  (1): BatchNorm1d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (upsample): SparseSequential(\n",
       "            (0): SparseInverseConv3d(64, 48, kernel_size=[2, 2, 2], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (decoder_blocks): SparseSequential(\n",
       "            (0): ResBlock(\n",
       "              (shortcut): SparseSequential(\n",
       "                (0): SubMConv3d(96, 48, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (conv1): SparseSequential(\n",
       "                (0): SubMConv3d(96, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (conv2): SparseSequential(\n",
       "                (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (shortcut): Identity()\n",
       "              (conv1): SparseSequential(\n",
       "                (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (conv2): SparseSequential(\n",
       "                (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "                (1): BatchNorm1d(48, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): SparseSequential(\n",
       "          (0): SparseInverseConv3d(48, 32, kernel_size=[2, 2, 2], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "          (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (decoder_blocks): SparseSequential(\n",
       "          (0): ResBlock(\n",
       "            (shortcut): SparseSequential(\n",
       "              (0): SubMConv3d(64, 32, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv1): SparseSequential(\n",
       "              (0): SubMConv3d(64, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv2): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResBlock(\n",
       "            (shortcut): Identity()\n",
       "            (conv1): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv2): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (upsample): SparseSequential(\n",
       "        (0): SparseInverseConv3d(32, 16, kernel_size=[2, 2, 2], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "        (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (decoder_blocks): SparseSequential(\n",
       "        (0): ResBlock(\n",
       "          (shortcut): SparseSequential(\n",
       "            (0): SubMConv3d(32, 16, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(32, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sem_seg_head): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (offset_head): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=16, out_features=3, bias=True)\n",
       "  )\n",
       "  (score_unet): SparseUNet(\n",
       "    (stem): SparseSequential(\n",
       "      (0): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (ublock): UBlock(\n",
       "      (encoder_blocks): SparseSequential(\n",
       "        (0): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): SparseSequential(\n",
       "        (0): SparseConv3d(16, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "        (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (ublock): UBlock(\n",
       "        (encoder_blocks): SparseSequential(\n",
       "          (0): ResBlock(\n",
       "            (shortcut): Identity()\n",
       "            (conv1): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv2): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResBlock(\n",
       "            (shortcut): Identity()\n",
       "            (conv1): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv2): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (upsample): SparseSequential(\n",
       "        (0): SparseInverseConv3d(32, 16, kernel_size=[2, 2, 2], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "        (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (decoder_blocks): SparseSequential(\n",
       "        (0): ResBlock(\n",
       "          (shortcut): SparseSequential(\n",
       "            (0): SubMConv3d(32, 16, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(32, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (score_head): Linear(in_features=16, out_features=9, bias=True)\n",
       "  (npcs_unet): SparseUNet(\n",
       "    (stem): SparseSequential(\n",
       "      (0): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (ublock): UBlock(\n",
       "      (encoder_blocks): SparseSequential(\n",
       "        (0): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): SparseSequential(\n",
       "        (0): SparseConv3d(16, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "        (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (ublock): UBlock(\n",
       "        (encoder_blocks): SparseSequential(\n",
       "          (0): ResBlock(\n",
       "            (shortcut): Identity()\n",
       "            (conv1): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv2): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResBlock(\n",
       "            (shortcut): Identity()\n",
       "            (conv1): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (conv2): SparseSequential(\n",
       "              (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "              (1): BatchNorm1d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (upsample): SparseSequential(\n",
       "        (0): SparseInverseConv3d(32, 16, kernel_size=[2, 2, 2], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "        (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (decoder_blocks): SparseSequential(\n",
       "        (0): ResBlock(\n",
       "          (shortcut): SparseSequential(\n",
       "            (0): SubMConv3d(32, 16, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(32, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (shortcut): Identity()\n",
       "          (conv1): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (npcs_head): Linear(in_features=16, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpv_net = test_GPV().cuda()\n",
    "gpv_net.load_state_dict(torch.load(\"log_dir/GPV_test_new_loss/2024-06-09 13:38:03.887472/GPV_[100|100].pth\"))\n",
    "gpv_net.eval()\n",
    "\n",
    "flownet = FlowNet3D().cuda()\n",
    "flownet.load_state_dict(torch.load(\"/home/zhangran/desktop/GithubClone/flownet3d_pytorch/pretrained_model/model.best.t7\"))\n",
    "flownet.eval()\n",
    "\n",
    "ins_seg = InsSegTest().cuda()\n",
    "ins_seg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error1:  3.6076375282294797\n",
      "error2:  2.68672604237751\n"
     ]
    }
   ],
   "source": [
    "pc_pairs = dataset_train[19]\n",
    "pc_pairs = [pc_pairs.to(\"cuda:0\")]\n",
    "with torch.no_grad():\n",
    "    (p_green_R1, p_red_R1, f_green_R1, f_red_R1), (p_green_R2, p_red_R2, f_green_R2, f_red_R2) = gpv_net(pc_pairs)\n",
    "    rot_1_pred = vectors_to_rotation_matrix(p_green_R1, p_red_R1, True)\n",
    "    rot_2_pred = vectors_to_rotation_matrix(p_green_R2, p_red_R2, True)\n",
    "    error1 = calculate_pose_metrics(rot_1_pred, torch.stack([pc_pairs[0].rot_1.transpose(0,1)]))\n",
    "    print(\"error1: \", error1)\n",
    "    error2 = calculate_pose_metrics(rot_2_pred, torch.stack([pc_pairs[0].rot_2.transpose(0,1)]))\n",
    "    print(\"error2: \", error2)\n",
    "    input1 = (rot_1_pred[0].transpose(0,1) @ pc_pairs[0].pc1.points[0:,0:3].transpose(0,1)).transpose(0,1)\n",
    "    feat1 = pc_pairs[0].pc1.points[0:,3:6]\n",
    "    input2 = (rot_2_pred[0].transpose(0,1) @ pc_pairs[0].pc2.points[0:,0:3].transpose(0,1)).transpose(0,1)\n",
    "    feat2 = pc_pairs[0].pc2.points[0:,3:6]\n",
    "    output = flownet(\n",
    "        input1.unsqueeze(0).transpose(1,2).contiguous(),\n",
    "        input2.unsqueeze(0).transpose(1,2).contiguous(),\n",
    "        feat1.unsqueeze(0).transpose(1,2).contiguous(),\n",
    "        feat2.unsqueeze(0).transpose(1,2).contiguous()\n",
    "    )\n",
    "    sem_preds, sem_logits, offsets_preds, proposals, score_logits, npcs_logits = ins_seg(pc_pairs[0].pc1.points[:,0:3].unsqueeze(0), output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ins_seg_loss(nn.Module):\n",
    "    def __init__(self, use_sem_focal_loss: bool = True, \n",
    "                 use_sem_dice_loss: bool = True, \n",
    "                 ignore_sem_label: int = -100, \n",
    "                 symmetry_indices: List[int] = [0, 1, 3, 3, 2, 0, 3, 2, 4, 1],\n",
    "                 train_schedule = [5, 10],\n",
    "                 device = \"cuda:0\"):\n",
    "        super(ins_seg_loss, self).__init__()\n",
    "        self.device = device\n",
    "        self.start_clustering_epoch = train_schedule[0]\n",
    "        self.start_npcs_epoch = train_schedule[1]\n",
    "        self.use_sem_focal_loss = use_sem_focal_loss\n",
    "        self.use_sem_dice_loss = use_sem_dice_loss\n",
    "        self.ignore_sem_label = ignore_sem_label\n",
    "        (\n",
    "            symmetry_matrix_1, symmetry_matrix_2, symmetry_matrix_3\n",
    "        ) = get_symmetry_matrix()\n",
    "        self.symmetry_matrix_1 = symmetry_matrix_1\n",
    "        self.symmetry_matrix_2 = symmetry_matrix_2\n",
    "        self.symmetry_matrix_3 = symmetry_matrix_3\n",
    "        self.symmetry_indices = torch.as_tensor(symmetry_indices, dtype=torch.int64).to(self.device)\n",
    "        \n",
    "        \n",
    "    def loss_sem_seg(\n",
    "        self,\n",
    "        sem_logits: torch.Tensor,\n",
    "        sem_labels: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        if self.use_sem_focal_loss:\n",
    "            loss = focal_loss(\n",
    "                sem_logits, sem_labels,\n",
    "                alpha=None,\n",
    "                gamma=2.0,\n",
    "                ignore_index=self.ignore_sem_label,\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "        else:\n",
    "            loss = F.cross_entropy(\n",
    "                sem_logits, sem_labels,\n",
    "                weight=None,\n",
    "                ignore_index=self.ignore_sem_label,\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "\n",
    "        if self.use_sem_dice_loss:\n",
    "            loss += dice_loss(\n",
    "                sem_logits[:, :, None, None], sem_labels[:, None, None],\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def loss_offset(\n",
    "        self,\n",
    "        offsets: torch.Tensor,\n",
    "        gt_offsets: torch.Tensor,\n",
    "        sem_labels: torch.Tensor,\n",
    "        instance_labels: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        valid_instance_mask = (sem_labels > 0) & (instance_labels >= 0)\n",
    "\n",
    "        pt_diff = offsets - gt_offsets\n",
    "        pt_dist = torch.sum(pt_diff.abs(), dim=-1)\n",
    "        loss_offset_dist = pt_dist[valid_instance_mask].mean()\n",
    "\n",
    "        gt_offsets_norm = torch.norm(gt_offsets, p=2, dim=-1)\n",
    "        gt_offsets = gt_offsets / (gt_offsets_norm[:, None] + 1e-8)\n",
    "\n",
    "        offsets_norm = torch.norm(offsets, p=2, dim=-1)\n",
    "        offsets = offsets / (offsets_norm[:, None] + 1e-8)\n",
    "\n",
    "        dir_diff = -(gt_offsets * offsets).sum(-1)\n",
    "        loss_offset_dir = dir_diff[valid_instance_mask].mean()\n",
    "\n",
    "        return loss_offset_dist, loss_offset_dir\n",
    "    \n",
    "    def loss_proposal_score(\n",
    "        self,\n",
    "        score_logits: torch.Tensor,\n",
    "        proposals: Instances,\n",
    "        num_points_per_instance: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        ious = batch_instance_seg_iou(\n",
    "            proposals.proposal_offsets, # type: ignore\n",
    "            proposals.instance_labels, # type: ignore\n",
    "            proposals.batch_indices, # type: ignore\n",
    "            num_points_per_instance,\n",
    "        )\n",
    "        proposals.ious = ious\n",
    "        proposals.num_points_per_instance = num_points_per_instance\n",
    "\n",
    "        ious_max = ious.max(-1)[0]\n",
    "        gt_scores = get_gt_scores(ious_max, 0.75, 0.25)\n",
    "\n",
    "        return F.binary_cross_entropy_with_logits(score_logits, gt_scores)\n",
    "    \n",
    "    def loss_proposal_npcs(\n",
    "        self,\n",
    "        npcs_logits: torch.Tensor,\n",
    "        gt_npcs: torch.Tensor,\n",
    "        proposals: Instances,\n",
    "    ) -> torch.Tensor:\n",
    "        sem_preds, sem_labels = proposals.sem_preds, proposals.sem_labels\n",
    "        proposal_indices = proposals.proposal_indices\n",
    "        valid_mask = (sem_preds == sem_labels) & (gt_npcs != 0).any(dim=-1)\n",
    "\n",
    "        npcs_logits = npcs_logits[valid_mask]\n",
    "        gt_npcs = gt_npcs[valid_mask]\n",
    "        sem_preds = sem_preds[valid_mask].long()\n",
    "        sem_labels = sem_labels[valid_mask]\n",
    "        proposal_indices = proposal_indices[valid_mask]\n",
    "\n",
    "        npcs_logits = rearrange(npcs_logits, \"n (k c) -> n k c\", c=3)\n",
    "        npcs_logits = npcs_logits.gather(\n",
    "            1, index=repeat(sem_preds - 1, \"n -> n one c\", one=1, c=3)\n",
    "        ).squeeze(1)\n",
    "\n",
    "        proposals.npcs_preds = npcs_logits.detach()\n",
    "        proposals.gt_npcs = gt_npcs\n",
    "        proposals.npcs_valid_mask = valid_mask\n",
    "\n",
    "        loss_npcs = 0\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        self.symmetry_indices = self.symmetry_indices.to(sem_preds.device)\n",
    "        self.symmetry_matrix_1 = self.symmetry_matrix_1.to(sem_preds.device)\n",
    "        self.symmetry_matrix_2 = self.symmetry_matrix_2.to(sem_preds.device)\n",
    "        self.symmetry_matrix_3 = self.symmetry_matrix_3.to(sem_preds.device)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        symmetry_indices = self.symmetry_indices[sem_preds]\n",
    "        # group #1\n",
    "        group_1_mask = symmetry_indices < 3\n",
    "        symmetry_indices_1 = symmetry_indices[group_1_mask]\n",
    "        if symmetry_indices_1.shape[0] > 0:\n",
    "            loss_npcs += compute_npcs_loss(\n",
    "                npcs_logits[group_1_mask], gt_npcs[group_1_mask],\n",
    "                proposal_indices[group_1_mask],\n",
    "                self.symmetry_matrix_1[symmetry_indices_1]\n",
    "            )\n",
    "\n",
    "        # group #2\n",
    "        group_2_mask = symmetry_indices == 3\n",
    "        symmetry_indices_2 = symmetry_indices[group_2_mask]\n",
    "        if symmetry_indices_2.shape[0] > 0:\n",
    "            loss_npcs += compute_npcs_loss(\n",
    "                npcs_logits[group_2_mask], gt_npcs[group_2_mask],\n",
    "                proposal_indices[group_2_mask],\n",
    "                self.symmetry_matrix_2[symmetry_indices_2 - 3]\n",
    "            )\n",
    "\n",
    "        # group #3\n",
    "        group_3_mask = symmetry_indices == 4\n",
    "        symmetry_indices_3 = symmetry_indices[group_3_mask]\n",
    "        if symmetry_indices_3.shape[0] > 0:\n",
    "            loss_npcs += compute_npcs_loss(\n",
    "                npcs_logits[group_3_mask], gt_npcs[group_3_mask],\n",
    "                proposal_indices[group_3_mask],\n",
    "                self.symmetry_matrix_3[symmetry_indices_3 - 4]\n",
    "            )\n",
    "\n",
    "        return loss_npcs\n",
    "    \n",
    "    def add_labels(self, pc_pairs, sem_preds, proposals):\n",
    "        batch_size = len(pc_pairs)\n",
    "        num_instances = [pc.pc1.num_instances for pc in pc_pairs]\n",
    "        max_num_instances = max(num_instances)\n",
    "        sem_labels = torch.cat([pc_pair.pc1.sem_labels for pc_pair in pc_pairs], dim=0)\n",
    "        instance_labels = torch.cat([pc_pair.pc1.instance_labels for pc_pair in pc_pairs], dim=0)\n",
    "        if proposals is None:\n",
    "            return sem_labels, instance_labels\n",
    "        proposals.sem_labels = sem_labels[proposals.valid_mask][\n",
    "            proposals.sorted_indices\n",
    "        ]\n",
    "        proposals.instance_labels = instance_labels[proposals.valid_mask][proposals.sorted_indices]\n",
    "        proposals.sem_preds = sem_preds[proposals.valid_mask][proposals.sorted_indices]\n",
    "        num_points_per_instance = torch.zeros(\n",
    "            batch_size, max_num_instances, dtype=torch.int32, device=self.device\n",
    "        )\n",
    "        instance_sem_labels = torch.full(\n",
    "            (batch_size, max_num_instances), -1, dtype=torch.int32, device=self.device\n",
    "        )\n",
    "        for i, pc in enumerate(pc_pairs):\n",
    "            num_points_per_instance[i, :pc.pc1.num_instances] = pc.pc1.num_points_per_instance \n",
    "            instance_sem_labels[i, :pc.pc1.num_instances] = pc.pc1.instance_sem_labels \n",
    "        proposals.num_points_per_instance = num_points_per_instance\n",
    "        proposals.instance_sem_labels = instance_sem_labels\n",
    "\n",
    "        return sem_labels, instance_labels\n",
    "    \n",
    "    def forward(self, epoch, sem_logits, sem_labels, instance_labels, offsets_preds, proposals, score_logits, npcs_logits, pc_pairs):\n",
    "        # self.add_labels(pc_pairs, sem_preds, proposals)\n",
    "        pt_xyz = torch.cat([pc_pair.pc1.points[:, :3] for pc_pair in pc_pairs], dim=0)\n",
    "        instance_regions = torch.cat([pc_pair.pc1.instance_regions for pc_pair in pc_pairs], dim=0)\n",
    "        gt_offsets = instance_regions[:, :3] - pt_xyz\n",
    "        gt_npcs = torch.cat([pc_pair.pc1.gt_npcs for pc_pair in pc_pairs], dim=0)\n",
    "        loss_sem = self.loss_sem_seg(sem_logits, sem_labels)\n",
    "        loss_offset_dist, loss_offset_dir = self.loss_offset(offsets_preds, gt_offsets, sem_labels, instance_labels)\n",
    "        if proposals is None:\n",
    "            return loss_sem + loss_offset_dist + loss_offset_dir\n",
    "        loss_score = self.loss_proposal_score(score_logits, proposals, proposals.num_points_per_instance)\n",
    "        gt_npcs = gt_npcs[proposals.valid_mask][proposals.sorted_indices]\n",
    "        loss_npcs = self.loss_proposal_npcs(npcs_logits, gt_npcs, proposals)\n",
    "        if epoch < self.start_clustering_epoch:\n",
    "            return loss_sem + loss_offset_dist + loss_offset_dir\n",
    "        elif epoch < self.start_npcs_epoch:\n",
    "            return loss_sem + loss_offset_dist + loss_offset_dir + loss_score\n",
    "        else:\n",
    "            return loss_sem + loss_offset_dist + loss_offset_dir + loss_score + loss_npcs\n",
    "        \n",
    "\n",
    "def train(model: test_GPV, dataloader_train, dataloader_test_inter, dataloader_test_intra, lr, num_epochs, log_dir):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = fs_net_loss_full()\n",
    "    name_list = ['Rot1', 'Rot2', 'Rot1_cos', 'Rot2_cos', 'Rot_regular']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    # model = DataParallel(model)\n",
    "    model.train()\n",
    "    log_dir = log_dir + \"/\" + str(datetime.today())\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    global_step = 0\n",
    "    print(\"_________________________train_epoch___________________________\")\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        if epoch == 0:\n",
    "            # first test epoch\n",
    "            print(\"______________________first_test_epoch_________________________\")\n",
    "            torch.save(model.state_dict(), log_dir+r'/'+f\"GPV_[{epoch+1}|{num_epochs}].pth\")\n",
    "            test_metrics(model, dataloader_test_inter, device, writer, epoch, 'test_inter')\n",
    "            test_metrics(model, dataloader_test_intra, device, writer, epoch, 'test_intra')\n",
    "            test_metrics(model, dataloader_train, device, writer, epoch, 'test_train')\n",
    "        for batch_idx, batch in enumerate(dataloader_train):\n",
    "            pc_pairs = [pair.to(device) for pair in batch]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            (p_green_R1, p_red_R1, f_green_R1, f_red_R1), (p_green_R2, p_red_R2, f_green_R2, f_red_R2) = model(pc_pairs)\n",
    "            \n",
    "            # Assuming we have ground truth rotations\n",
    "            R_green_gt1, R_red_gt1 = get_gt_v(ground_truth_rotations([pc.rot_1.T for pc in pc_pairs]))  # Function to get ground truth rotation vectors\n",
    "            R_green_gt2, R_red_gt2 = get_gt_v(ground_truth_rotations([pc.rot_2.T for pc in pc_pairs]))  # Function to get ground truth rotation vectors\n",
    "            \n",
    "            pred_list1 = {\n",
    "                \"Rot1\": p_green_R1,\n",
    "                \"Rot2\": p_red_R1,\n",
    "            }\n",
    "            gt_list1 = {\n",
    "                \"Rot1\": R_green_gt1.cuda(),\n",
    "                \"Rot2\": R_red_gt1.cuda(),\n",
    "            }\n",
    "            \n",
    "            pred_list2 = {\n",
    "                \"Rot1\": p_green_R2,\n",
    "                \"Rot2\": p_red_R2,\n",
    "            }\n",
    "            gt_list2 = {\n",
    "                \"Rot1\": R_green_gt2.cuda(),\n",
    "                \"Rot2\": R_red_gt2.cuda(),\n",
    "            }\n",
    "\n",
    "            sym1, sym2 = get_sym_from_input(pc_pairs)\n",
    "\n",
    "            loss_dict1 = criterion(name_list, pred_list1, gt_list1, sym1)\n",
    "            loss_dict2 = criterion(name_list, pred_list2, gt_list2, sym2)\n",
    "            loss = (sum(loss_dict1.values()) + sum(loss_dict2.values())) / 2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # 每10个batch记录一次loss\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                writer.add_scalar('train/loss', loss.item(), global_step)\n",
    "                print(f\"Epoch:[{epoch + 1}|{num_epochs}],Batch:[{(batch_idx + 1)}|{len(dataloader_train)}],Loss:[{loss.item():.4f}]\")\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader_train)\n",
    "        print(f\"Epoch [{epoch+1}|{num_epochs}],Loss:{avg_loss:.4f}\")\n",
    "        writer.add_scalar('train/avg_loss', avg_loss, epoch)\n",
    "\n",
    "        # 每10个epoch跑一次测试集\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), log_dir+r'/'+f\"GPV_[{epoch+1}|{num_epochs}].pth\")\n",
    "            test_metrics(model, dataloader_test_inter, device, writer, epoch, 'test_inter')\n",
    "            test_metrics(model, dataloader_test_intra, device, writer, epoch, 'test_intra')\n",
    "            test_metrics(model, dataloader_train, device, writer, epoch, 'test_train')\n",
    "\n",
    "\n",
    "def test_metrics(model, dataloader, device, writer, epoch, phase):\n",
    "    print(\"______________________\" + phase + \"_______________________\")\n",
    "    model.eval()\n",
    "    all_pred_rot_matrices = []\n",
    "    all_gt_rot_matrices = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            pc_pairs = [pair.to(device) for pair in batch]\n",
    "            (p_green_R1, p_red_R1, f_green_R1, f_red_R1), (p_green_R2, p_red_R2, f_green_R2, f_red_R2) = model(pc_pairs)\n",
    "            \n",
    "            # Assuming we have ground truth rotations\n",
    "            R_green_gt1, R_red_gt1 = get_gt_v(ground_truth_rotations([pc.rot_1.T for pc in pc_pairs]))  # Function to get ground truth rotation vectors\n",
    "            R_green_gt2, R_red_gt2 = get_gt_v(ground_truth_rotations([pc.rot_2.T for pc in pc_pairs]))  # Function to get ground truth rotation vectors\n",
    "            \n",
    "            # Convert predicted vectors and ground truth vectors back to rotation matrices\n",
    "            pred_rot_matrices1 = vectors_to_rotation_matrix(p_green_R1, p_red_R1, True)\n",
    "            pred_rot_matrices2 = vectors_to_rotation_matrix(p_green_R2, p_red_R2, True)\n",
    "            gt_rot_matrices1 = vectors_to_rotation_matrix(R_green_gt1, R_red_gt1, True)\n",
    "            gt_rot_matrices2 = vectors_to_rotation_matrix(R_green_gt2, R_red_gt2, True)\n",
    "            \n",
    "            # Store predictions and ground truths for metrics calculation\n",
    "            all_pred_rot_matrices.append(pred_rot_matrices1.cpu())\n",
    "            all_pred_rot_matrices.append(pred_rot_matrices2.cpu())\n",
    "            all_gt_rot_matrices.append(gt_rot_matrices1.cpu())\n",
    "            all_gt_rot_matrices.append(gt_rot_matrices2.cpu())\n",
    "    \n",
    "    all_pred_rot_matrices = torch.cat(all_pred_rot_matrices, dim=0)\n",
    "    all_gt_rot_matrices = torch.cat(all_gt_rot_matrices, dim=0)\n",
    "\n",
    "    mean_rot_error = calculate_pose_metrics(\n",
    "        all_pred_rot_matrices, all_gt_rot_matrices\n",
    "    )\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(f'{phase}/mean_rot_error', mean_rot_error, epoch)\n",
    "    print(f\"{phase} - Epoch [{epoch+1}]: Mean Rotation Error: {mean_rot_error:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "def train_ins_seg(ins_seg, gpv_net, flownet, dataloader_train, dataloader_test_intra, dataloader_test_inter, num_epochs, lr, train_schedule, log_dir, device):\n",
    "    optimizer = torch.optim.Adam(ins_seg.parameters(), lr=lr)\n",
    "    ins_seg.train()\n",
    "    gpv_net.eval()\n",
    "    flownet.eval() # fix flownet parameters\n",
    "    ins_seg = ins_seg.to(device)\n",
    "    gpv_net = gpv_net.to(device)\n",
    "    flownet = flownet.to(device)\n",
    "    criterion = ins_seg_loss(train_schedule=train_schedule)\n",
    "    log_dir = log_dir + \"/\" + str(datetime.today())\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    print(\"_________________________train_epoch___________________________\")\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_all_accu = 0\n",
    "        total_pixel_accu = 0\n",
    "        if epoch == 0:\n",
    "            print(\"______________________first_test_epoch_________________________\")\n",
    "            torch.save(ins_seg.state_dict(), log_dir+r'/'+f\"GPV_[{epoch+1}|{num_epochs}].pth\")\n",
    "            test_ins_seg(ins_seg, gpv_net, flownet, dataloader_test_inter, device, writer, epoch, 'test_inter', criterion)\n",
    "            test_ins_seg(ins_seg, gpv_net, flownet, dataloader_test_intra, device, writer, epoch, 'test_intra', criterion)\n",
    "            test_ins_seg(ins_seg, gpv_net, flownet, dataloader_train, device, writer, epoch, 'test_train', criterion)\n",
    "        for batch_idx, batch in enumerate(dataloader_train):\n",
    "            bs = len(batch)\n",
    "            pc_pairs = [pair.to(device) for pair in batch]\n",
    "            (p_green_R1, p_red_R1, f_green_R1, f_red_R1), (p_green_R2, p_red_R2, f_green_R2, f_red_R2) = gpv_net(pc_pairs)            \n",
    "            # Convert predicted vectors and ground truth vectors back to rotation matrices\n",
    "            pred_rot_matrices1 = vectors_to_rotation_matrix(p_green_R1, p_red_R1, True)\n",
    "            pred_rot_matrices2 = vectors_to_rotation_matrix(p_green_R2, p_red_R2, True)\n",
    "            input1 = torch.stack([(pred_rot_matrices1[i].transpose(0,1) @ pc_pairs[i].pc1.points[:,0:3].transpose(0,1)).transpose(0,1) for i in range(bs)], dim=0)\n",
    "            feat1 = torch.stack([pc_pair.pc1.points[:,3:6] for pc_pair in pc_pairs], dim=0)\n",
    "            input2 = torch.stack([(pred_rot_matrices2[i].transpose(0,1) @ pc_pairs[i].pc2.points[:,0:3].transpose(0,1)).transpose(0,1) for i in range(bs)], dim=0)\n",
    "            feat2 = torch.stack([pc_pair.pc2.points[:,3:6] for pc_pair in pc_pairs], dim=0)\n",
    "            flow_data = flownet(\n",
    "                input1.transpose(1,2).contiguous(),\n",
    "                input2.transpose(1,2).contiguous(),\n",
    "                feat1.transpose(1,2).contiguous(),\n",
    "                feat2.transpose(1,2).contiguous()\n",
    "            )\n",
    "            pt_xyz = torch.cat([pc_pair.pc1.points[:,0:3].unsqueeze(0) for pc_pair in pc_pairs], dim=0)\n",
    "            instance_labels = torch.cat([pc_pair.pc1.instance_labels for pc_pair in pc_pairs], dim=0)\n",
    "            flow_data = torch.cat([pc_pair.pc1.points[:,3:6].unsqueeze(0) for pc_pair in pc_pairs], dim=0).permute(0,2,1)\n",
    "            # points = torch.cat([pt_xyz, flow_data.permute(0, 2, 1)], dim=2)\n",
    "            # cat_inputs = torch.cat([points[i] for i in range(points.shape[0])], dim=0)\n",
    "            sem_preds, sem_logits, offsets_preds, proposals, score_logits, npcs_logits = ins_seg(pt_xyz, flow_data, instance_labels)\n",
    "            sem_labels, instance_labels = criterion.add_labels(pc_pairs, sem_preds, proposals)\n",
    "            all_accu = (sem_preds == sem_labels).sum().float() / (sem_labels.shape[0])\n",
    "            instance_mask = sem_labels > 0\n",
    "            pixel_accu = pixel_accuracy(sem_preds[instance_mask], sem_labels[instance_mask])\n",
    "            # criterion.add_labels(pc_pairs, sem_preds, proposals)\n",
    "            loss = criterion(epoch, sem_logits, sem_labels, instance_labels, offsets_preds, proposals, score_logits, npcs_logits, pc_pairs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            total_loss += loss.item()\n",
    "            total_all_accu += all_accu.item()\n",
    "            total_pixel_accu += pixel_accu\n",
    "            # 每10个batch记录一次loss\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                writer.add_scalar('train/loss', loss.item(), global_step)\n",
    "                writer.add_scalar('train/all_accu', all_accu.item(), global_step)\n",
    "                writer.add_scalar('train/pixel_accu', pixel_accu, global_step)\n",
    "                print(f\"Epoch:[{epoch + 1}|{num_epochs}],Batch:[{(batch_idx + 1)}|{len(dataloader_train)}],Loss:[{loss.item():.4f}]\")\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader_train)\n",
    "        avg_all_accu = total_all_accu / len(dataloader_train)\n",
    "        avg_pixel_accu = total_pixel_accu / len(dataloader_train)\n",
    "        print(f\"Epoch [{epoch+1}|{num_epochs}],Loss:{avg_loss:.4f}\")\n",
    "        writer.add_scalar('train/avg_loss', avg_loss, epoch)\n",
    "        writer.add_scalar('train/avg_all_accu', avg_all_accu * 100, epoch)\n",
    "        writer.add_scalar('train/avg_pixel_accu', avg_pixel_accu * 100, epoch)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(ins_seg.state_dict(), log_dir+r'/'+f\"GPV_[{epoch+1}|{num_epochs}].pth\")\n",
    "            test_ins_seg(ins_seg, gpv_net, flownet, dataloader_test_inter, device, writer, epoch, 'test_inter', criterion)\n",
    "            test_ins_seg(ins_seg, gpv_net, flownet, dataloader_test_intra, device, writer, epoch, 'test_intra', criterion)\n",
    "            test_ins_seg(ins_seg, gpv_net, flownet, dataloader_train, device, writer, epoch, 'test_train', criterion)\n",
    "\n",
    "def test_ins_seg(ins_seg, gpv_net, flownet, dataloader, device, writer, epoch, phase, criterion):\n",
    "    print(\"______________________\" + phase + \"_______________________\")\n",
    "    ins_seg.eval()\n",
    "    gpv_net.eval()\n",
    "    flownet.eval()\n",
    "    all_sem_preds = []\n",
    "    all_sem_labels = []\n",
    "    all_pred_rot_matrices = []\n",
    "    all_gt_rot_matrices = []\n",
    "    all_proposals = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_all_accu = 0\n",
    "        total_pixel_accu = 0\n",
    "        for batch in tqdm(dataloader):\n",
    "            bs = len(batch)\n",
    "            pc_pairs = [pair.to(device) for pair in batch]\n",
    "            (p_green_R1, p_red_R1, f_green_R1, f_red_R1), (p_green_R2, p_red_R2, f_green_R2, f_red_R2) = gpv_net(pc_pairs)\n",
    "            pred_rot_matrices1 = vectors_to_rotation_matrix(p_green_R1, p_red_R1, True)\n",
    "            pred_rot_matrices2 = vectors_to_rotation_matrix(p_green_R2, p_red_R2, True)\n",
    "            all_pred_rot_matrices.append(pred_rot_matrices1.cpu())\n",
    "            all_pred_rot_matrices.append(pred_rot_matrices2.cpu())\n",
    "            R_green_gt1, R_red_gt1 = get_gt_v(ground_truth_rotations([pc.rot_1.T for pc in pc_pairs]))  # Function to get ground truth rotation vectors\n",
    "            R_green_gt2, R_red_gt2 = get_gt_v(ground_truth_rotations([pc.rot_2.T for pc in pc_pairs]))\n",
    "            gt_rot_matrices1 = vectors_to_rotation_matrix(R_green_gt1, R_red_gt1, True)\n",
    "            gt_rot_matrices2 = vectors_to_rotation_matrix(R_green_gt2, R_red_gt2, True)\n",
    "            all_gt_rot_matrices.append(gt_rot_matrices1.cpu())\n",
    "            all_gt_rot_matrices.append(gt_rot_matrices2.cpu())\n",
    "            input1 = torch.stack([(pred_rot_matrices1[i].transpose(0,1) @ pc_pairs[i].pc1.points[:,0:3].transpose(0,1)).transpose(0,1) for i in range(bs)], dim=0)\n",
    "            feat1 = torch.stack([pc_pair.pc1.points[:,3:6] for pc_pair in pc_pairs], dim=0)\n",
    "            input2 = torch.stack([(pred_rot_matrices2[i].transpose(0,1) @ pc_pairs[i].pc2.points[:,0:3].transpose(0,1)).transpose(0,1) for i in range(bs)], dim=0)\n",
    "            feat2 = torch.stack([pc_pair.pc2.points[:,3:6] for pc_pair in pc_pairs], dim=0)\n",
    "            flow_data = flownet(\n",
    "                input1.transpose(1,2).contiguous(),\n",
    "                input2.transpose(1,2).contiguous(),\n",
    "                feat1.transpose(1,2).contiguous(),\n",
    "                feat2.transpose(1,2).contiguous()\n",
    "            )\n",
    "            pt_xyz = torch.cat([pc_pair.pc1.points[:,0:3].unsqueeze(0) for pc_pair in pc_pairs], dim=0)\n",
    "            instance_labels = torch.cat([pc_pair.pc1.instance_labels for pc_pair in pc_pairs], dim=0)\n",
    "            flow_data = torch.cat([pc_pair.pc1.points[:,3:6].unsqueeze(0) for pc_pair in pc_pairs], dim=0).permute(0,2,1)\n",
    "            # points = torch.cat([pt_xyz, flow_data.permute(0, 2, 1)], dim=2)\n",
    "            # cat_inputs = torch.cat([points[i] for i in range(points.shape[0])], dim=0)\n",
    "            sem_preds, sem_logits, offsets_preds, proposals, score_logits, npcs_logits = ins_seg(pt_xyz, flow_data, instance_labels)\n",
    "            sem_labels, instance_labels = criterion.add_labels(pc_pairs, sem_preds, proposals)\n",
    "            loss = criterion(epoch, sem_logits, sem_labels, instance_labels, offsets_preds, proposals, score_logits, npcs_logits, pc_pairs)\n",
    "            all_accu = (sem_preds == sem_labels).sum().float() / (sem_labels.shape[0])\n",
    "            instance_mask = sem_labels > 0\n",
    "            pixel_accu = pixel_accuracy(sem_preds[instance_mask], sem_labels[instance_mask])\n",
    "            total_loss += loss.item()\n",
    "            total_all_accu += all_accu.item()\n",
    "            total_pixel_accu += pixel_accu\n",
    "            all_sem_preds.append(sem_preds)\n",
    "            all_sem_labels.append(sem_labels)\n",
    "            if proposals is not None:\n",
    "                # proposals = filter_invalid_proposals(\n",
    "                #     proposals,\n",
    "                #     score_threshold=0.09,\n",
    "                #     min_num_points_per_proposal=3\n",
    "                # )\n",
    "                # proposals = apply_nms(proposals, 0.3)\n",
    "                proposals.pt_sem_classes = proposals.sem_preds[proposals.proposal_offsets[:-1].long()]\n",
    "                all_proposals.append(proposals)\n",
    "    all_sem_preds = torch.cat(all_sem_preds, dim=0)\n",
    "    all_sem_labels = torch.cat(all_sem_labels, dim=0)\n",
    "    miou = mean_iou(all_sem_preds, all_sem_labels, num_classes=10)\n",
    "    thes = [0.5 + 0.05 * i for i in range(10)]\n",
    "    aps = []\n",
    "    for the in thes:\n",
    "        if len(all_proposals) != 0:\n",
    "            ap = compute_ap(all_proposals, 10, the)\n",
    "        else:\n",
    "            ap = None\n",
    "        if ap is not None:\n",
    "            aps.append(ap)\n",
    "        if the == 0.5:\n",
    "            ap50 = ap\n",
    "    mAP = np.array(aps)\n",
    "    # 消除nan后再评价\n",
    "    mAP = np.nanmean(mAP) if len(aps) != 0 else 0\n",
    "    all_pred_rot_matrices = torch.cat(all_pred_rot_matrices, dim=0)\n",
    "    all_gt_rot_matrices = torch.cat(all_gt_rot_matrices, dim=0)\n",
    "    mean_rot_error = calculate_pose_metrics(\n",
    "        all_pred_rot_matrices, all_gt_rot_matrices\n",
    "    )\n",
    "    mean_all_accu = total_all_accu / len(dataloader)\n",
    "    mean_pixel_accu = total_pixel_accu / len(dataloader)\n",
    "    # print result\n",
    "    print(f\"{phase} - Epoch [{epoch+1}]: Mean Rotation Error: {mean_rot_error:.4f}\")\n",
    "    print(f\"{phase} - Epoch [{epoch+1}]: Mean AP@50: {np.nanmean(ap50) * 100 if ap50 is not None else 0:.4f}\")\n",
    "    print(f\"{phase} - Epoch [{epoch+1}]: Mean mAP: {mAP * 100:.4f}\")\n",
    "    print(f\"{phase} - Epoch [{epoch+1}]: Mean mIoU: {miou * 100:.4f}\")\n",
    "    print(f\"{phase} - Epoch [{epoch+1}]: Mean All Accu: {mean_all_accu * 100:.4f}\")\n",
    "    print(f\"{phase} - Epoch [{epoch+1}]: Mean Pixel Accu: {mean_pixel_accu * 100:.4f}\")\n",
    "    # record results\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(f'{phase}/mean_rot_error', mean_rot_error, epoch)\n",
    "        writer.add_scalar(\n",
    "            f\"{phase}/mean_AP@50\",\n",
    "            np.nanmean(ap50) * 100 if ap50 is not None else 0,\n",
    "            epoch\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            f\"{phase}/mAP\",\n",
    "            mAP * 100,\n",
    "            epoch\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            f\"{phase}/mIoU\",\n",
    "            miou * 100,\n",
    "            epoch\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            f\"{phase}/all_accu\",\n",
    "            mean_all_accu * 100,\n",
    "            epoch\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            f\"{phase}/all_accu\",\n",
    "            mean_pixel_accu * 100,\n",
    "            epoch\n",
    "        )\n",
    "        for class_idx in range(1, 10):\n",
    "            partname = PART_ID2NAME[class_idx]\n",
    "            writer.add_scalar(\n",
    "                f\"{phase}/AP@50_{partname}\",\n",
    "                np.nanmean(ap50[class_idx - 1]) * 100 if ap50 is not None else 0,\n",
    "                epoch\n",
    "            )\n",
    "    ins_seg.train()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________train_epoch___________________________\n",
      "______________________first_test_epoch_________________________\n",
      "______________________test_inter_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "/tmp/ipykernel_1253701/1537311927.py:530: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(ap50[class_idx - 1]) * 100 if ap50 is not None else 0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inter - Epoch [1]: Mean Rotation Error: 13.4973\n",
      "test_inter - Epoch [1]: Mean AP@50: 0.0000\n",
      "test_inter - Epoch [1]: Mean mAP: 0.0000\n",
      "test_inter - Epoch [1]: Mean mIoU: 60.0000\n",
      "test_inter - Epoch [1]: Mean All Accu: 0.0000\n",
      "test_inter - Epoch [1]: Mean Pixel Accu: 0.0000\n",
      "______________________test_intra_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_intra - Epoch [1]: Mean Rotation Error: 23.8796\n",
      "test_intra - Epoch [1]: Mean AP@50: 0.0000\n",
      "test_intra - Epoch [1]: Mean mAP: 0.0000\n",
      "test_intra - Epoch [1]: Mean mIoU: 50.0000\n",
      "test_intra - Epoch [1]: Mean All Accu: 0.0000\n",
      "test_intra - Epoch [1]: Mean Pixel Accu: 0.0000\n",
      "______________________test_train_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train - Epoch [1]: Mean Rotation Error: 3.3025\n",
      "test_train - Epoch [1]: Mean AP@50: 0.0000\n",
      "test_train - Epoch [1]: Mean mAP: 0.0000\n",
      "test_train - Epoch [1]: Mean mIoU: 20.0000\n",
      "test_train - Epoch [1]: Mean All Accu: 0.0000\n",
      "test_train - Epoch [1]: Mean Pixel Accu: 0.0000\n",
      "Epoch [1|400],Loss:4.1797\n",
      "Epoch [2|400],Loss:3.3869\n",
      "Epoch [3|400],Loss:3.0267\n",
      "Epoch [4|400],Loss:2.7712\n",
      "Epoch [5|400],Loss:2.5757\n",
      "Epoch [6|400],Loss:2.3889\n",
      "Epoch [7|400],Loss:2.2266\n",
      "Epoch [8|400],Loss:2.0867\n",
      "Epoch [9|400],Loss:1.9656\n",
      "Epoch [10|400],Loss:1.8525\n",
      "______________________test_inter_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inter - Epoch [10]: Mean Rotation Error: 5.8953\n",
      "test_inter - Epoch [10]: Mean AP@50: 0.0000\n",
      "test_inter - Epoch [10]: Mean mAP: 0.0000\n",
      "test_inter - Epoch [10]: Mean mIoU: 13.2705\n",
      "test_inter - Epoch [10]: Mean All Accu: 31.2558\n",
      "test_inter - Epoch [10]: Mean Pixel Accu: 0.1808\n",
      "______________________test_intra_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_intra - Epoch [10]: Mean Rotation Error: 23.9349\n",
      "test_intra - Epoch [10]: Mean AP@50: 0.0000\n",
      "test_intra - Epoch [10]: Mean mAP: 0.0000\n",
      "test_intra - Epoch [10]: Mean mIoU: 6.8915\n",
      "test_intra - Epoch [10]: Mean All Accu: 70.7267\n",
      "test_intra - Epoch [10]: Mean Pixel Accu: 0.0000\n",
      "______________________test_train_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train - Epoch [10]: Mean Rotation Error: 3.3662\n",
      "test_train - Epoch [10]: Mean AP@50: 0.0000\n",
      "test_train - Epoch [10]: Mean mAP: 0.0000\n",
      "test_train - Epoch [10]: Mean mIoU: 26.8046\n",
      "test_train - Epoch [10]: Mean All Accu: 69.2150\n",
      "test_train - Epoch [10]: Mean Pixel Accu: 0.0000\n",
      "Epoch [11|400],Loss:1.7499\n",
      "Epoch [12|400],Loss:1.6560\n",
      "Epoch [13|400],Loss:1.5682\n",
      "Epoch [14|400],Loss:1.5100\n",
      "Epoch [15|400],Loss:1.4390\n",
      "Epoch [16|400],Loss:1.3846\n",
      "Epoch [17|400],Loss:1.3340\n",
      "Epoch [18|400],Loss:1.2919\n",
      "Epoch [19|400],Loss:1.2547\n",
      "Epoch [20|400],Loss:1.2225\n",
      "______________________test_inter_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inter - Epoch [20]: Mean Rotation Error: 14.0189\n",
      "test_inter - Epoch [20]: Mean AP@50: 0.0000\n",
      "test_inter - Epoch [20]: Mean mAP: 0.0000\n",
      "test_inter - Epoch [20]: Mean mIoU: 12.7123\n",
      "test_inter - Epoch [20]: Mean All Accu: 21.2429\n",
      "test_inter - Epoch [20]: Mean Pixel Accu: 2.7365\n",
      "______________________test_intra_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_intra - Epoch [20]: Mean Rotation Error: 22.8031\n",
      "test_intra - Epoch [20]: Mean AP@50: 0.0000\n",
      "test_intra - Epoch [20]: Mean mAP: 0.0000\n",
      "test_intra - Epoch [20]: Mean mIoU: 4.3194\n",
      "test_intra - Epoch [20]: Mean All Accu: 34.2365\n",
      "test_intra - Epoch [20]: Mean Pixel Accu: 0.7737\n",
      "______________________test_train_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train - Epoch [20]: Mean Rotation Error: 3.4382\n",
      "test_train - Epoch [20]: Mean AP@50: 0.0000\n",
      "test_train - Epoch [20]: Mean mAP: 0.0000\n",
      "test_train - Epoch [20]: Mean mIoU: 16.7750\n",
      "test_train - Epoch [20]: Mean All Accu: 68.3771\n",
      "test_train - Epoch [20]: Mean Pixel Accu: 0.1136\n",
      "Epoch [21|400],Loss:1.1955\n",
      "Epoch [22|400],Loss:1.1702\n",
      "Epoch [23|400],Loss:1.1495\n",
      "Epoch [24|400],Loss:1.1267\n",
      "Epoch [25|400],Loss:1.1073\n",
      "Epoch [26|400],Loss:1.0905\n",
      "Epoch [27|400],Loss:1.0749\n",
      "Epoch [28|400],Loss:1.0599\n",
      "Epoch [29|400],Loss:1.0466\n",
      "Epoch [30|400],Loss:1.0356\n",
      "______________________test_inter_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inter - Epoch [30]: Mean Rotation Error: 5.6401\n",
      "test_inter - Epoch [30]: Mean AP@50: 0.0000\n",
      "test_inter - Epoch [30]: Mean mAP: 0.0000\n",
      "test_inter - Epoch [30]: Mean mIoU: 2.6269\n",
      "test_inter - Epoch [30]: Mean All Accu: 17.3248\n",
      "test_inter - Epoch [30]: Mean Pixel Accu: 4.7868\n",
      "______________________test_intra_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_intra - Epoch [30]: Mean Rotation Error: 24.6737\n",
      "test_intra - Epoch [30]: Mean AP@50: 0.0000\n",
      "test_intra - Epoch [30]: Mean mAP: 0.0000\n",
      "test_intra - Epoch [30]: Mean mIoU: 4.8444\n",
      "test_intra - Epoch [30]: Mean All Accu: 43.6310\n",
      "test_intra - Epoch [30]: Mean Pixel Accu: 1.0944\n",
      "______________________test_train_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train - Epoch [30]: Mean Rotation Error: 3.2856\n",
      "test_train - Epoch [30]: Mean AP@50: 0.0000\n",
      "test_train - Epoch [30]: Mean mAP: 0.0000\n",
      "test_train - Epoch [30]: Mean mIoU: 16.8544\n",
      "test_train - Epoch [30]: Mean All Accu: 69.0073\n",
      "test_train - Epoch [30]: Mean Pixel Accu: 0.8365\n",
      "Epoch [31|400],Loss:1.0252\n",
      "Epoch [32|400],Loss:1.0218\n",
      "Epoch [33|400],Loss:1.0312\n",
      "Epoch [34|400],Loss:1.0167\n",
      "Epoch [35|400],Loss:1.0055\n",
      "Epoch [36|400],Loss:0.9952\n",
      "Epoch [37|400],Loss:0.9835\n",
      "Epoch [38|400],Loss:0.9721\n",
      "Epoch [39|400],Loss:0.9639\n",
      "Epoch [40|400],Loss:0.9588\n",
      "______________________test_inter_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inter - Epoch [40]: Mean Rotation Error: 7.0589\n",
      "test_inter - Epoch [40]: Mean AP@50: 0.0000\n",
      "test_inter - Epoch [40]: Mean mAP: 0.0000\n",
      "test_inter - Epoch [40]: Mean mIoU: 2.5526\n",
      "test_inter - Epoch [40]: Mean All Accu: 16.9981\n",
      "test_inter - Epoch [40]: Mean Pixel Accu: 4.0964\n",
      "______________________test_intra_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_intra - Epoch [40]: Mean Rotation Error: 20.7527\n",
      "test_intra - Epoch [40]: Mean AP@50: 0.0000\n",
      "test_intra - Epoch [40]: Mean mAP: 0.0000\n",
      "test_intra - Epoch [40]: Mean mIoU: 4.8122\n",
      "test_intra - Epoch [40]: Mean All Accu: 44.0623\n",
      "test_intra - Epoch [40]: Mean Pixel Accu: 0.7734\n",
      "______________________test_train_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train - Epoch [40]: Mean Rotation Error: 3.4902\n",
      "test_train - Epoch [40]: Mean AP@50: 0.0000\n",
      "test_train - Epoch [40]: Mean mAP: 0.0000\n",
      "test_train - Epoch [40]: Mean mIoU: 26.8317\n",
      "test_train - Epoch [40]: Mean All Accu: 69.1354\n",
      "test_train - Epoch [40]: Mean Pixel Accu: 0.4253\n",
      "Epoch [41|400],Loss:0.9530\n",
      "Epoch [42|400],Loss:0.9485\n",
      "Epoch [43|400],Loss:0.9369\n",
      "Epoch [44|400],Loss:0.9291\n",
      "Epoch [45|400],Loss:0.9231\n",
      "Epoch [46|400],Loss:0.9150\n",
      "Epoch [47|400],Loss:0.9096\n",
      "Epoch [48|400],Loss:0.9087\n",
      "Epoch [49|400],Loss:0.9105\n",
      "Epoch [50|400],Loss:0.9126\n",
      "______________________test_inter_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inter - Epoch [50]: Mean Rotation Error: 12.1095\n",
      "test_inter - Epoch [50]: Mean AP@50: 0.0000\n",
      "test_inter - Epoch [50]: Mean mAP: 0.0000\n",
      "test_inter - Epoch [50]: Mean mIoU: 2.8725\n",
      "test_inter - Epoch [50]: Mean All Accu: 18.9469\n",
      "test_inter - Epoch [50]: Mean Pixel Accu: 6.4043\n",
      "______________________test_intra_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_intra - Epoch [50]: Mean Rotation Error: 15.2785\n",
      "test_intra - Epoch [50]: Mean AP@50: 0.0000\n",
      "test_intra - Epoch [50]: Mean mAP: 0.0000\n",
      "test_intra - Epoch [50]: Mean mIoU: 4.8972\n",
      "test_intra - Epoch [50]: Mean All Accu: 45.9802\n",
      "test_intra - Epoch [50]: Mean Pixel Accu: 0.4789\n",
      "______________________test_train_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train - Epoch [50]: Mean Rotation Error: 3.2598\n",
      "test_train - Epoch [50]: Mean AP@50: 0.0000\n",
      "test_train - Epoch [50]: Mean mAP: 0.0000\n",
      "test_train - Epoch [50]: Mean mIoU: 36.8298\n",
      "test_train - Epoch [50]: Mean All Accu: 69.1408\n",
      "test_train - Epoch [50]: Mean Pixel Accu: 0.3947\n",
      "Epoch [51|400],Loss:0.9072\n",
      "Epoch [52|400],Loss:0.9004\n",
      "Epoch [53|400],Loss:0.8944\n",
      "Epoch [54|400],Loss:0.8897\n",
      "Epoch [55|400],Loss:0.8889\n",
      "Epoch [56|400],Loss:0.8866\n",
      "Epoch [57|400],Loss:0.8811\n",
      "Epoch [58|400],Loss:0.8652\n",
      "Epoch [59|400],Loss:0.8589\n",
      "Epoch [60|400],Loss:0.8531\n",
      "______________________test_inter_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inter - Epoch [60]: Mean Rotation Error: 8.9631\n",
      "test_inter - Epoch [60]: Mean AP@50: 0.0000\n",
      "test_inter - Epoch [60]: Mean mAP: 0.0000\n",
      "test_inter - Epoch [60]: Mean mIoU: 2.8592\n",
      "test_inter - Epoch [60]: Mean All Accu: 19.0575\n",
      "test_inter - Epoch [60]: Mean Pixel Accu: 5.9604\n",
      "______________________test_intra_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_intra - Epoch [60]: Mean Rotation Error: 25.1440\n",
      "test_intra - Epoch [60]: Mean AP@50: 0.0000\n",
      "test_intra - Epoch [60]: Mean mAP: 0.0000\n",
      "test_intra - Epoch [60]: Mean mIoU: 4.9687\n",
      "test_intra - Epoch [60]: Mean All Accu: 47.2202\n",
      "test_intra - Epoch [60]: Mean Pixel Accu: 0.3994\n",
      "______________________test_train_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train - Epoch [60]: Mean Rotation Error: 3.2554\n",
      "test_train - Epoch [60]: Mean AP@50: 0.0000\n",
      "test_train - Epoch [60]: Mean mAP: 0.0000\n",
      "test_train - Epoch [60]: Mean mIoU: 36.8280\n",
      "test_train - Epoch [60]: Mean All Accu: 69.1544\n",
      "test_train - Epoch [60]: Mean Pixel Accu: 0.3568\n",
      "Epoch [61|400],Loss:0.8463\n",
      "Epoch [62|400],Loss:0.8435\n",
      "Epoch [63|400],Loss:0.8509\n",
      "Epoch [64|400],Loss:0.8414\n",
      "Epoch [65|400],Loss:0.8385\n",
      "Epoch [66|400],Loss:0.8437\n",
      "Epoch [67|400],Loss:0.8400\n",
      "Epoch [68|400],Loss:0.8249\n",
      "Epoch [69|400],Loss:0.8153\n",
      "Epoch [70|400],Loss:0.8127\n",
      "______________________test_inter_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inter - Epoch [70]: Mean Rotation Error: 6.1447\n",
      "test_inter - Epoch [70]: Mean AP@50: 0.0000\n",
      "test_inter - Epoch [70]: Mean mAP: 0.0000\n",
      "test_inter - Epoch [70]: Mean mIoU: 3.2041\n",
      "test_inter - Epoch [70]: Mean All Accu: 20.8140\n",
      "test_inter - Epoch [70]: Mean Pixel Accu: 10.0820\n",
      "______________________test_intra_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_intra - Epoch [70]: Mean Rotation Error: 20.3942\n",
      "test_intra - Epoch [70]: Mean AP@50: 0.0000\n",
      "test_intra - Epoch [70]: Mean mAP: 0.0000\n",
      "test_intra - Epoch [70]: Mean mIoU: 4.8709\n",
      "test_intra - Epoch [70]: Mean All Accu: 46.1035\n",
      "test_intra - Epoch [70]: Mean Pixel Accu: 0.3471\n",
      "______________________test_train_______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_train - Epoch [70]: Mean Rotation Error: 3.0692\n",
      "test_train - Epoch [70]: Mean AP@50: 0.0000\n",
      "test_train - Epoch [70]: Mean mAP: 0.0000\n",
      "test_train - Epoch [70]: Mean mIoU: 36.8342\n",
      "test_train - Epoch [70]: Mean All Accu: 69.1462\n",
      "test_train - Epoch [70]: Mean Pixel Accu: 0.4066\n",
      "Epoch [71|400],Loss:0.8083\n",
      "Epoch [72|400],Loss:0.7994\n",
      "Epoch [73|400],Loss:0.7906\n",
      "Epoch [74|400],Loss:0.7902\n",
      "Epoch [75|400],Loss:0.7904\n",
      "Epoch [76|400],Loss:0.7874\n",
      "Epoch [77|400],Loss:0.7854\n",
      "Epoch [78|400],Loss:0.7822\n",
      "Epoch [79|400],Loss:0.7827\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 32])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_ins_seg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpv_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflownet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_test_intra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_test_inter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_dir/ins_seg_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 370\u001b[0m, in \u001b[0;36mtrain_ins_seg\u001b[0;34m(ins_seg, gpv_net, flownet, dataloader_train, dataloader_test_intra, dataloader_test_inter, num_epochs, lr, train_schedule, log_dir, device)\u001b[0m\n\u001b[1;32m    367\u001b[0m flow_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([pc_pair\u001b[38;5;241m.\u001b[39mpc1\u001b[38;5;241m.\u001b[39mpoints[:,\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pc_pair \u001b[38;5;129;01min\u001b[39;00m pc_pairs], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# points = torch.cat([pt_xyz, flow_data.permute(0, 2, 1)], dim=2)\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# cat_inputs = torch.cat([points[i] for i in range(points.shape[0])], dim=0)\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m sem_preds, sem_logits, offsets_preds, proposals, score_logits, npcs_logits \u001b[38;5;241m=\u001b[39m \u001b[43mins_seg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt_xyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m sem_labels, instance_labels \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39madd_labels(pc_pairs, sem_preds, proposals)\n\u001b[1;32m    372\u001b[0m all_accu \u001b[38;5;241m=\u001b[39m (sem_preds \u001b[38;5;241m==\u001b[39m sem_labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m (sem_labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 116\u001b[0m, in \u001b[0;36mInsSegTest.forward\u001b[0;34m(self, points, flow_data, instance_labels)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proposals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sem_preds, sem_logits, offsets_preds, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m score_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_proposal_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvoxel_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpc_voxel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m proposal_offsets_begin \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mproposal_offsets[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    120\u001b[0m score_logits \u001b[38;5;241m=\u001b[39m score_logits\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;241m1\u001b[39m, proposals\u001b[38;5;241m.\u001b[39msem_preds[proposal_offsets_begin]\u001b[38;5;241m.\u001b[39mlong()[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    122\u001b[0m )\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 164\u001b[0m, in \u001b[0;36mInsSegTest.forward_proposal_score\u001b[0;34m(self, voxel_tensor, pc_voxel_id, proposals)\u001b[0m\n\u001b[1;32m    161\u001b[0m proposal_offsets_begin \u001b[38;5;241m=\u001b[39m proposal_offsets[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    162\u001b[0m proposal_offsets_end \u001b[38;5;241m=\u001b[39m proposal_offsets[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m score_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoxel_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m score_features \u001b[38;5;241m=\u001b[39m score_features\u001b[38;5;241m.\u001b[39mfeatures[pc_voxel_id]\n\u001b[1;32m    166\u001b[0m pooled_score_features, _ \u001b[38;5;241m=\u001b[39m segmented_maxpool(\n\u001b[1;32m    167\u001b[0m     score_features, proposal_offsets_begin, proposal_offsets_end\n\u001b[1;32m    168\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/desktop/myProject/MBPN/network/gap_layers.py:136\u001b[0m, in \u001b[0;36mSparseUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m--> 136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mublock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/desktop/myProject/MBPN/network/gap_layers.py:116\u001b[0m, in \u001b[0;36mUBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 116\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mublock(x)\n\u001b[1;32m    118\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/spconv/pytorch/modules.py:142\u001b[0m, in \u001b[0;36mSparseSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, spconv\u001b[38;5;241m.\u001b[39mSparseConvTensor):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_feature(\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 32])"
     ]
    }
   ],
   "source": [
    "train_ins_seg(ins_seg, gpv_net, flownet, dataloader_train, dataloader_test_intra, dataloader_test_inter, 400, 0.001, [100, 200], \"log_dir/ins_seg_test\", \"cuda:0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
